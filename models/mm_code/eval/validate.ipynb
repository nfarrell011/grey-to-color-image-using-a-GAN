{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128a84b-045e-4bf2-87f7-a767c4735173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from generator import UNet\n",
    "from gan_utils_new import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7375ddf-d1fe-40bc-a6b0-84906cf1711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "604298e0-86da-4746-aab7-7e3c9668d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "def preprocess_image(image_path):\n",
    "    # Load and resize the image\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = img.resize((256, 256))\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    img_lab = rgb2lab(np.array(img)).astype(\"float32\")\n",
    "    \n",
    "    # Normalize L channel to range [-1, 1]\n",
    "    L = img_lab[..., 0:1] / 50.0 - 1.0\n",
    "    \n",
    "    # ab channels should be between [-1, 1]\n",
    "    ab = img_lab[..., 1:] / 110.0\n",
    "    \n",
    "    # Convert to tensors\n",
    "    L = torch.tensor(L).permute(2, 0, 1).unsqueeze(0)  # (1, 1, 256, 256)\n",
    "    ab = torch.tensor(ab).permute(2, 0, 1).unsqueeze(0)  # (1, 2, 256, 256)\n",
    "    \n",
    "    return L, ab, img_lab\n",
    "\n",
    "# Inference on L channel\n",
    "def run_inference_on_L(model, L):\n",
    "    with torch.no_grad():\n",
    "        ab_pred = model(L)\n",
    "    return ab_pred\n",
    "\n",
    "# Recompile the LAB image and convert back to RGB\n",
    "def reassemble_and_convert_to_rgb(L, ab_pred):\n",
    "    # Denormalize L and ab channels\n",
    "    L = (L.squeeze(0).squeeze(0).cpu().numpy() + 1.0) * 50.0  # back to [0, 100] range\n",
    "    ab_pred = ab_pred.squeeze(0).cpu().numpy() * 110.0  # back to [-110, 110] range\n",
    "    \n",
    "    # Reassemble LAB image\n",
    "    lab_pred = np.concatenate([L[..., np.newaxis], ab_pred.transpose(1, 2, 0)], axis=-1)\n",
    "    \n",
    "    # Convert LAB to RGB\n",
    "    rgb_pred = lab2rgb(lab_pred)\n",
    "    return rgb_pred\n",
    "\n",
    "# Visualize the images\n",
    "def visualize_images(original_img, reconstructed_img):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    ax[0].imshow(original_img)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].imshow(reconstructed_img)\n",
    "    ax[1].set_title(\"Reconstructed Image\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0aace90d-1d53-4453-939a-c037bf1f2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "model = UNet()\n",
    "path_to_weights = \"/home/massone.m/image_enhancement/training_runs/test_17/model_weights/generator_weights.pth\"\n",
    "checkpoint = torch.load(path_to_weights)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()  # Set the model to inference mode\n",
    "\n",
    "# Load data\n",
    "coco_path = \"/home/massone.m/image_enhancement/train_sample\"\n",
    "paths = glob.glob(coco_path + \"/*.jpg\") \n",
    "\n",
    "# Get val data\n",
    "num_imgs = 1000\n",
    "split = 0.8\n",
    "train_paths, val_paths = select_images(paths, num_imgs, split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c08362e-6fe4-4fdd-bfe3-864559949e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "for image_path in val_paths:\n",
    "\n",
    "    # Preprocess the image\n",
    "    L, ab, original_lab = preprocess_image(image_path)\n",
    "    \n",
    "    # Run inference\n",
    "    ab_pred = run_inference_on_L(model, L)\n",
    "    \n",
    "    # Reassemble and convert to RGB\n",
    "    reconstructed_img = reassemble_and_convert_to_rgb(L, ab_pred)\n",
    "    \n",
    "    # Convert original LAB back to RGB for comparison\n",
    "    original_rgb = lab2rgb(original_lab)\n",
    "    \n",
    "    # Visualize original and reconstructed images\n",
    "    visualize_images(original_rgb, reconstructed_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc25e1c-fbaf-42af-967f-15958891ee49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb5512a-cd2e-435d-9044-a0dd903068bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
